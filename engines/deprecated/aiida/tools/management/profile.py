"""
Tools for inspecting the AiiDA profile (database statistics, groups).
"""
import os
import io
import json      # ğŸš© è¡¥ä¸Šè¿™ä¸ª
import zipfile   # ğŸš© è¡¥ä¸Šè¿™ä¸ª
from pathlib import Path
from aiida import load_profile, orm
from aiida.orm import Group, Node, QueryBuilder, ProcessNode, Node
from aiida.manage.configuration import get_config
from aiida.manage.manager import get_manager
from aiida.storage.sqlite_zip.backend import SqliteZipBackend

# ğŸš© å¢åŠ ä¸€ä¸ªå†…å­˜ç¼“å­˜ï¼Œè®°å½•å½“å‰åŠ è½½çš„ Archive è·¯å¾„
_CURRENT_MOUNTED_ARCHIVE = None

# --- 1. èµ„æºåˆ—è¡¨å·¥å…· (Perceptor å¼ºä¾èµ–) ---

def ensure_environment(target: str):
    """
    æ™ºèƒ½åˆ‡æ¢ç¯å¢ƒï¼šè‡ªåŠ¨è¯†åˆ«æ˜¯æœ¬åœ° Profile è¿˜æ˜¯ Archive æ–‡ä»¶ã€‚
    """
    global _CURRENT_MOUNTED_ARCHIVE

    if not target or target == "(None)":
        return

    if target == _CURRENT_MOUNTED_ARCHIVE:
        return

    try:
        # 1. å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ä¸”å­˜åœ¨
        if os.path.isfile(target) and target.lower().endswith(('.aiida', '.zip')):
            # ğŸš€ æ ¸å¿ƒä¿®å¤ï¼šå°† Archive æ–‡ä»¶è·¯å¾„åŒ…è£…æˆä¸´æ—¶ Profile å¯¹è±¡
            archive_profile = SqliteZipBackend.create_profile(filepath=target,)
            load_profile(archive_profile, allow_switch=True)
            _CURRENT_MOUNTED_ARCHIVE = target # æ›´æ–°ç¼“å­˜
            print(f"âœ… Backend loaded archive as profile: {target}")
        else:
            # 2. å¦åˆ™æŒ‰æ™®é€š Profile åç§°åŠ è½½
            load_profile(target, allow_switch=True)
            _CURRENT_MOUNTED_ARCHIVE = None # åˆ‡æ¢å›æ™®é€š Profile
            print(f"âœ… Backend switched to profile: {target}")
    except Exception as e:
        print(f"âŒ DEBUG: Failed to switch AiiDA environment: {e}")

def list_system_profiles():
    """
    è·å–ç³»ç»Ÿä¸­æ‰€æœ‰ AiiDA Profile çš„åç§°åˆ—è¡¨ã€‚
    (ä¿®å¤äº†æ„ŸçŸ¥å™¨æ‰¾ä¸åˆ°è¯¥å‡½æ•°çš„é—®é¢˜)
    """
    try:
        return [p.name for p in get_config().profiles]
    except Exception as e:
        logger.warning(f"AiiDA config not found or invalid: {e}")
        return []

def list_local_archives():
    """
    æ‰«æå½“å‰ç›®å½•ä¸‹çš„ AiiDA å‹ç¼©åŒ…æ–‡ä»¶ã€‚
    æ”¯æŒ .aiida å’Œ .zip æ ¼å¼ã€‚
    """
    return [f.name for f in Path('.').glob('*') if f.suffix in ['.aiida', '.zip']]

# --- 2. ç¯å¢ƒåˆ‡æ¢å·¥å…· ---

def switch_profile(profile_name: str) -> str:
    """
    åˆ‡æ¢å½“å‰çš„ AiiDA Profileã€‚
    """
    available = list_system_profiles()
    if profile_name not in available:
        return f"Error: Profile '{profile_name}' not found. Available: {available}"
        
    try:
        load_profile(profile_name, allow_switch=True)
        return f"Successfully switched to profile '{profile_name}'."
    except Exception as e:
        return f"Error switching profile: {e}"

def load_archive_profile(filepath: str):
    """
    å°†å‹ç¼©åŒ…ä½œä¸ºä¸´æ—¶ Profile åŠ è½½ï¼ˆä¸»è¦ç”¨äº AiiDA 2.x çš„åªè¯»æ¢æµ‹ï¼‰ã€‚
    """
    try:
        from aiida.storage.sqlite_zip.backend import SqliteZipBackend
        archive_profile = SqliteZipBackend.create_profile(filepath = filepath)
        load_profile(archive_profile, allow_switch=True)
        # è¿™é‡Œçš„å®ç°å–å†³äºä½ çš„å…·ä½“ç¯å¢ƒé…ç½®ï¼Œé€šå¸¸å»ºè®®ç›´æ¥é€šè¿‡ get_archive_info æ¢æµ‹
        # å¦‚æœéœ€è¦å®Œæ•´åŠ è½½ï¼Œé€šå¸¸ä½¿ç”¨ä¸´æ—¶å­˜å‚¨åç«¯
        return f"Archive profile loading for '{filepath}' is ready for implementation."
    except Exception as e:
        return f"Error loading archive: {e}"

# --- 3. æ·±åº¦æ„ŸçŸ¥å·¥å…· (Unified Map) ---
def get_unified_source_map(target: str):
    """
    ç»Ÿä¸€èµ„æºæ˜ å°„é€»è¾‘ï¼šå…ˆå¼ºåˆ¶åŒæ­¥ç¯å¢ƒï¼Œå†ç”¨ QueryBuilder è¯»å–ã€‚
    """
    ensure_environment(target)
    
    # ğŸš© ä¿®å¤ KeyError: å¢åŠ  'type' é”®
    is_arch = target.lower().endswith(('.aiida', '.zip'))
    result = {
        "name": os.path.basename(target), 
        "type": "archive" if is_arch else "profile", 
        "groups": []
    }
    try:
        # ç¯å¢ƒä¸€æ—¦åŒæ­¥ï¼Œç»Ÿä¸€ä½¿ç”¨ ORM æŸ¥è¯¢
        qb = orm.QueryBuilder().append(orm.Group, project=["label", "id"])
        for label, pk in qb.all():
            if "import" in label.lower(): continue
            result["groups"].append({"label": label, "pk": pk})
    except Exception as e:
        result["error"] = str(e)
    return result

# --- 4. æ•°æ®ç»Ÿè®¡å·¥å…· ---

def get_statistics(profile_name: str = None):
    """
    è·å–æ•°æ®åº“çš„é«˜å±‚ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    if profile_name:
        switch_profile(profile_name)
            
    output = io.StringIO()
    output.write(f"=== Database Stats ({get_manager().get_profile().name}) ===\n")
    
    types = {
        "Calculations": "process.calculation.calcjob.CalcJobNode.",
        "WorkChains": "process.workflow.workchain.WorkChainNode.",
        "Structures": "data.core.structure.StructureData."
    }
    
    for name, node_type in types.items():
        count = QueryBuilder().append(Node, filters={"node_type": {"like": f"{node_type}%"}}).count()
        output.write(f"{name}: {count}\n")
        
    return output.getvalue()

def list_groups(search_string: str = None):
    """
    ä»¥ Markdown è¡¨æ ¼å½¢å¼åˆ—å‡ºæ‰€æœ‰ç»„ï¼Œå¯¹ AI éå¸¸å‹å¥½ã€‚
    """
    qb = QueryBuilder()
    filters = {"label": {"like": f"%{search_string}%"}} if search_string else {}
    qb.append(Group, project=["label", "id", "*"], filters=filters)
    
    current = get_manager().get_profile().name
    lines = [f"**Groups in Profile: `{current}`**", "", "| PK | Label | Count |", "| :--- | :--- | :--- |"]
    
    for label, pk, group in qb.all():
        if group.type_string == "core.import": continue
        lines.append(f"| {pk} | {label} | {len(group.nodes)} |")
    
    return "\n".join(lines)

def get_database_summary():
    """
    ä¸“é—¨ä¸º UI è¿å®¾ç•Œé¢è®¾è®¡çš„å¿«é€Ÿç»Ÿè®¡å·¥å…·ã€‚
    è¿”å›åŸå§‹æ•°æ®å­—å…¸ï¼Œä¾› UI ä½¿ç”¨ã€‚
    """
    try:
        n_count = QueryBuilder().append(Node).count()
        p_count = QueryBuilder().append(ProcessNode).count()
        
        # è¿˜å¯ä»¥é¡ºä¾¿ç»Ÿè®¡ä¸€ä¸‹å¤±è´¥çš„ä»»åŠ¡
        failed_count = orm.QueryBuilder().append(
            ProcessNode, 
            filters={'exit_status': {'!==': 0}}
        ).count()

        return {
            "status": "success",
            "node_count": n_count,
            "process_count": p_count,
            "failed_count": failed_count
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

def get_recent_processes(limit: int = 5):
    """
    ğŸš© æ ¸å¿ƒï¼šå°è£… AiiDA æ•°æ®åº“æŸ¥è¯¢é€»è¾‘ã€‚
    è¿™ä¸ªå‡½æ•°æ—¢å¯ä»¥ç»™ AI å½“ Tool ç”¨ï¼Œä¹Ÿå¯ä»¥ç»™ Controller å½“å†…éƒ¨æ•°æ®æºç”¨ã€‚
    """
    qb = QueryBuilder()
    qb.append(ProcessNode, project=['id', 'attributes.process_state', 'attributes.process_label', 'ctime'], tag='process')
    qb.order_by({'process': {'ctime': 'desc'}})
    qb.limit(limit)
    
    results = []
    for pk, state, label, ctime in qb.all():
        results.append({
            'pk': pk,
            'state': state.value if hasattr(state, 'value') else str(state),
            'label': label or 'Unknown Task'
        })
    return results